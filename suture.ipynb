{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_path = r''\n",
    "\n",
    "## for running the second round of percentile based cleaning\n",
    "#os.makedirs(os.path.join(TOP_PATH, 'round 2')\n",
    "#SAVE_PATH = os.path.join(TOP_PATH, 'round 2')\n",
    "#IMGS = tuple(np.load(os.path.join(TOP_PATH, 'INDEXED test.npy'), allow_pickle=True))\n",
    "#NAMES = tuple(np.load(os.path.join(TOP_PATH, 'NAMES test.npy'), allow_pickle=True))\n",
    "#print(len(IMGS), len(NAMES))\n",
    "\n",
    "str_filter = 'orientation'\n",
    "file_type = '.tif'\n",
    "process_type = 'float'\n",
    "\n",
    "str_trim = len(file_type)\n",
    "files = os.listdir(top_path)\n",
    "\n",
    "arrs = []\n",
    "names = []\n",
    "for i in files:\n",
    "    if i.endswith(str_filter):\n",
    "        names.append(i[:-str_trim])\n",
    "        if 'retardance' in i:\n",
    "            arrs.append(imread(os.path.join(top_path,i)))\n",
    "        elif 'orientation' in i:\n",
    "            if process_type == 'float':\n",
    "                arrs.append(np.around((io.imread(os.path.join(TOP_PATH,i))/100).astype(np.float), decimals=3))\n",
    "            elif process_type == 'int':\n",
    "                arrs.append((io.imread(os.path.join(TOP_PATH,i))/100).astype(np.uint8))\n",
    "            else:\n",
    "                print('preprocessing info incomplete')\n",
    "        else:\n",
    "            print('unknown image type')\n",
    "    else:\n",
    "        continue\n",
    "names = tuple(names)\n",
    "arrs = tuple(arrs)\n",
    "\n",
    "for i in arrs:\n",
    "    plt.imshow(i, cmap = 'hsv')\n",
    "    plt.show() \n",
    "\n",
    "##--------------------------------------------------------------------------------------\n",
    "    \n",
    "percentiles, pairs, filt_imgs, px_ct, ob_ct, l_imgs = [], [], [], [], [], []\n",
    "for i in arrs:\n",
    "    t = tuple(np.percentile(i, np.arange(0,101,5)).astype(float)) #based on frequency of angles in image\n",
    "    #t = tuple(np.arange(0,181,2).astype(int)) #fixed angle bin width\n",
    "    #t = tuple(np.arange(0,np.amax(i),10).astype(int)) #fixed angle bin width retardance\n",
    "    percentiles.append(t)\n",
    "    p = tuple([(t[j], t[j+1]) for j in range(len(t)-1)])\n",
    "    paris.append(p)\n",
    "    f_i = []\n",
    "    px = []\n",
    "    ob = []\n",
    "    l_i = []\n",
    "    for j in p:\n",
    "        a = np.copy(i)\n",
    "        a[i <= j[0]] = 0\n",
    "        a[i > j[1]] = 0\n",
    "        f_i.append(a)\n",
    "        l = label(binary_opening(a, structure=square(3)), structure=square(3))\n",
    "        l_i.append(l[0])\n",
    "        ob.append(l[1])\n",
    "        px.append(np.sum(a.astype(bool).astype(int)))\n",
    "    filt_imgs.append(tuple(f_i))\n",
    "    px_ct.append(tuple(px))\n",
    "    ob_ct.append(tuple(ob))\n",
    "    l_imgs.append(tuple(l_i))\n",
    "percentiles, pairs, filt_imgs, px_ct, ob_ct, l_imgs = (tuple(percentiles), \n",
    "                                                       tuple(pairs), tuple(filt_imgs), \n",
    "                                                       tuple(px_ct), \n",
    "                                                       tuple(ob_ct), \n",
    "                                                       tuple(l_imgs))\n",
    "\n",
    "##--------------------------------------------------------------------------------------\n",
    "\n",
    "sizes, masks, indexed, tissues = [], [], [], []\n",
    "for i in range(len(l_imgs)):\n",
    "    i_img = l_imgs[i]\n",
    "    \n",
    "    stack = np.sum(np.stack(i_img), axis = 0)\n",
    "    masks.append(stack.astype(np.uint8))\n",
    "    a = np.copy(imgs[i])\n",
    "    a[stack == False] = 0\n",
    "    indexed.append(a)\n",
    "    tissues.append(label(a, structure=square(3)))\n",
    "    \n",
    "    s1 = []\n",
    "    for j in range(i_img):\n",
    "        j_img = i_img[j]\n",
    "        ids = np.arange(1, obj_ct[i][j]+1, 1)\n",
    "        s2 = []\n",
    "        for k in ids:\n",
    "            k_size = np.where(j_img.flatten() == k)[0].shape[0]\n",
    "            s2.append(k_size)\n",
    "        s2 = np.concatenate((np.expand_dims(ids, axis = 1), np.expand_dims(s2, axis = 1)), axis = 1)\n",
    "        s1.append(s2)\n",
    "    sizes.append(tuple(s1))\n",
    "sizes, masks, indexed, tissues = (tuple(sizes), tuple(masks), tuple(indexed), tuple(tissues))\n",
    "\n",
    "np.save(os.path.join(SAVE_PATH, 'names.npy'), NAMES, allow_pickle=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'imgs.npy'), IMGS, allow_pickle=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'percentiles.npy'), PERCENTILES, allow_pickle=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'pairs.npy'), PAIRS, allow_pickle=True)\n",
    "\n",
    "np.save(os.path.join(SAVE_PATH, 'filtered images.npy'), FILT_IMGS, allow_pickle=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'pixel count.npy'), PX_CT, allow_pickle=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'object count.npy'), OB_CT, allow_pickle=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'labeled images.npy'), L_IMGS, allow_pickle=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'object sizes.npy'), SIZES, allow_pickle=True)\n",
    "\n",
    "np.save(os.path.join(SAVE_PATH, 'masks.npy'), MASKS, allow_pickle=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'indexed.npy'), INDEXED, allow_pickle=True)\n",
    "np.save(os.path.join(SAVE_PATH, 'tissues.npy'), TISSUES, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
